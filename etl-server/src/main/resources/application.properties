logging.file=${etl.home}/log/etl.log

spring.application.name=etl
spring.datasource.url=jdbc:h2:mem:test_mem
spring.datasource.username=admin
spring.datasource.password=admin
spring.datasource.driver-class-name=org.h2.Driver

spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=update
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.H2Dialect
spring.batch.job.enabled=false

#staging configurations goes here
staging.dataset.localPath=${etl.home}/datasets
staging.dataset.remotePath=/user/${hadoop.user}/datasets
spark.drivers.staing.resource=file:///${etl.home}/../etl-spark/etl-spark-staging/target/etl-spark-staging.jar
spark.drivers.staing.mainClass=etl.spark.Application

#pipeline configurations goes here
spark.drivers.dq.resource=file:///${etl.home}/../etl-spark/etl-spark-pipeline/target/etl-spark-pipeline.jar
spark.drivers.dq.mainClass=etl.spark.Application

phoenix.datasource.url=jdbc:h2:mem:test_mem
phoenix.datasource.username=admin
phoenix.datasource.password=admin
phoenix.datasource.driver-class-name=org.h2.Driver

phoenix.saltBuckets=20
phoenix.client.thin.url=jdbc:phoenix:thin:url=http://localhost:8765;serialization=PROTOBUF
zookeeper.url=localhost:2181
hbase.rootDir=/hbase

hadoop.user=etl
hadoop.conf.dir=${etl.home}/src/test/resources/hadoop
spark.conf.dir=${etl.home}/src/test/resources/spark/conf
spark.jars.dir=E:/repos/spark-2.1.2-bin-hadoop2.7/jars


etl.home=c:/workspaces/etl/etl-server
server.port=8080
etl.host=localhost
job.staging.base.dir=${etl.home}/tmp