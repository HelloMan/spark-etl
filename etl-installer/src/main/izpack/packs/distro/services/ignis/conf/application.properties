spring.application.name=etl

spring.datasource.url=${database.jdbc.url}
spring.datasource.username=${database.app.user}
spring.datasource.password=${database.app.password}
spring.datasource.driver-class-name=${database.jdbc.driver}

spring.jpa.show-sql=true
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl
spring.jpa.properties.hibernate.dialect=${database.hibernate.dialect}

spring.batch.job.enabled=false
spring.batch.initializer.enabled=false

staging.dataset.localPath=${DATA_PATH}/datasets
staging.dataset.remotePath=/user/${hadoop.user}/datasets
spring.logging.file=${IGNIS_HOME}/log/etl.log

spark.drivers.dq.resource=file:///${etl.home}/lib/drivers/etl-spark-pipeline.jar
spark.drivers.dq.mainClass=etl.spark.Application
spark.drivers.staging.resource=file:///${etl.home}/lib/drivers/etl-spark-staging.jar
spark.drivers.staging.mainClass=etl.spark.Application

phoenix.datasource.url=${database.jdbc.url}
phoenix.datasource.username=${database.app.user}
phoenix.datasource.password=${database.app.password}
phoenix.datasource.driver-class-name=${database.jdbc.driver}

phoenix.saltBuckets=20
zookeeper.url=${etl.host}:2181
hbase.rootDir=/hbase

hadoop.user=${hadoop.user}
hadoop.conf.dir=${HADOOP_HOME}/etc/hadoop
spark.conf.dir=${SPARK_HOME}/conf/
spark.jars.dir=${SPARK_HOME}/jars

etl.home=${IGNIS_HOME}
etl.host=${etl.host}
server.port=${etl.http.port}

job.staging.base.dir=${etl.home}/tmp